# DA6401 Assignment 2

The objective of this assignment is to a CNN model from scratch and learn how to tune the hyperparameters and visualize filters and finetune a pre-trained model. This repository explores two complementary approaches for classifying a 10-class subset of the iNaturalist dataset:

1. **Part A â€“ Training from Scratch**  
   Design and train a fully configurable fiveâ€‘block convolutional neural network (CNN) from the ground up. Perform extensive hyperparameter sweeps with Weights & Biases (W&B) to identify optimal configurations, and visualize model behavior and explanations.

2. **Part B â€“ Fineâ€‘Tuning Preâ€‘Trained Models**  
   Leverage transfer learning by adapting EfficientNet V2 Small and ResNetâ€‘50 backbones. Experiment with different layerâ€‘freezing strategies, compare performance against the scratchâ€‘trained CNN, and log experiments to W&B.

>The implementation of this repository, and the detailed analysis is available in this [report](https://api.wandb.ai/links/ns25z040-indian-institute-of-technology-madras/81xj4n40).


---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ partA/                         # Training a CNN from scratch
â”‚   â”œâ”€â”€ models/                    # `CNNModel` LightningModule
â”‚   â”œâ”€â”€ utils/                     # Activations, data loader, visualizations
â”‚   â”œâ”€â”€ main.py                    # CLI: train & test, sweep integration
â”‚   â”œâ”€â”€ sweep.py                   # W&B hyperparameter sweep setup
â”‚   â”œâ”€â”€ requirements.txt           # Part A dependencies
â”‚   â””â”€â”€ README.md                  # Detailed docs for Part A

â”œâ”€â”€ partB/                         # Fineâ€‘tuning preâ€‘trained architectures
â”‚   â”œâ”€â”€ data_loader.py             # `INaturalistDataLoader` for stratified splits
â”‚   â”œâ”€â”€ finetune.py                # `FineTuneModel` LightningModule
â”‚   â”œâ”€â”€ train.py                   # CLI for fineâ€‘tuning experiments
â”‚   â”œâ”€â”€ test.py                    # CLI for checkpoint evaluation
â”‚   â”œâ”€â”€ requirements.txt           # Part B dependencies
â”‚   â”œâ”€â”€ save/                      # Autoâ€‘saved best checkpoints
â”‚   â””â”€â”€ README.md                  # Detailed docs for Part B

â””â”€â”€ README.md                      # This overarching introduction & guide
```

---

## Dependencies

1. **Clone the repository**
   ```bash
   git clone https://github.com/iikrithii/da6401_assignment2.git
   cd da6401_assignment2
   ```

2. **Install dependencies**
   - **Part A**:
     ```bash
     cd partA
     pip install -r requirements.txt
     ```
   - **Part B**:
     ```bash
     cd ../partB
     pip install -r requirements.txt
     ```

## Preparing Your Data

1. **Folder Structure**: Ensure your dataset sits in `<anypath>/data/`in the following format:
   ```text
   /data/train/   # class1/, class2/, â€¦
   /data/val/     # class1/, class2/, â€¦
   ```
2. **Image Requirements**:
   - Formats: `JPEG`, `PNG`, etc.
   - Minimum resolution: â‰¥128Ã—128 recommended.
3. **Class Balance**: Stratified splitting relies on having multiple samples per class. If a class has <5 images, consider merging or removing it.
4. **Val Split Percentage**: Default is 20% split of the **train/** folder for validation; adjust via the `val_split` argument in `INaturalistDataLoader` if needed.

## Guideleines for Implementing Code
   - **Part A**: See [partA/README.md](partA/README.md)
   - **Part B**: See [partB/README.md](partB/README.md)

---

## ğŸ“Š Results Summary

### Part A â€“ Scratchâ€‘trained CNN
- **Best Validation Accuracy**: 48.05%  
- **Test Accuracy**: ~44.5%  
- **Highlights**: Modular 5â€‘block CNN, W&B sweep revealed progressive filter depths, nonâ€‘linear activations (GELU/SILU/MISH) and moderate dropout rates (~0.04) worked best.

### Part B â€“ Transfer Learning
| Model                         | Val Acc. | Test Acc. |
|-------------------------------|----------|-----------|
| Scratch CNN (BS=16)          | 48.0%    | 44.0%     |
| ResNetâ€‘50 (BS=16)            | 77.0%    | 76.85%    |
| ResNetâ€‘50 (BS=32)            | 78.1%    | 76.05%    |
| EfficientNet V2 S (BS=16)    | 75.2%    | 73.3%     |
| EfficientNet V2 S (BS=32)    | 75.9%    | 74.1%     |

**Analysis**: Fineâ€‘tuning preâ€‘trained backbones yields a ~30â€¯ppt gain in validation accuracy over scratchâ€‘trained models, underscoring transfer learningâ€™s power for limited data.

---



